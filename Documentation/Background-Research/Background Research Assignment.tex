%-------------------------------------------------------------------------------
% File:		Homework.tex
% Author:	Igor Janjic, Brian Hilnbrand, Danny Duangphachanh, Leah
%		Krynitsky  
% Description:	[ECE 4534] Embedded Systems Design
%		Background Research Assignment
%%------------------------------------------------------------------------------ 

%\documentclass{Book}

\input{./Preamble.tex}
\input{./Definitions.tex}
\input{./Programming.tex}

\begin{document}

\input{./Title.tex}

\subsection*{Background}
Of the first autonomous robots, none are more famous than Grey Walter's tortoises Elmer and Elsie. These two robots were very simple analog circuits equipped with three wheels for locomotion and light sensors which they used to navigate their environment and move toward illuminated recharging stations when low on power \cite{Walter:1950}. Since then, the design of autonomous robots has shifted toward the processing of sensory data using algorithms based in terms of digital computation. The remaining paper focuses on this last view of autonomous robotics.

\subsection*{Robot Framework Considerations}
The three primitives of robotics: plan, sense, act, as well as their relationships imply a robotic framework. Since the design specifications and hardware limitations are given, the robotics framework must then be determined by judging each framework's strengths and weaknesses as well as the compatibility between the framework and the choice of available sensors (described later). Thus, we are purposefully limiting our discussion to only those frameworks that coincide with usability of the various sensors available.

The framework must be able to handle quickly processing large amounts of complex information.

Additionally, the framework must be able to handle the prioritization of tasks. Since the robot has multiple goals that it is trying to achieve, often at the same time and often contradictory to each other, the control system framework must be able to rank goals by priority and focus computational power to solving high-priority goals first while at the same time reserving enough computational power to service low-priority goals.

The more sensors the robot has, the more information it is able to gather and then process to help it achieve its goals. The framework must be able to handle error in the sensors, overlapping sensors, inconsistent or failed sensors, and the addition of more sensors, 

\subsection*{Layered Control Paradigm \cite{Brooks:1985}}
This paradigm was designed by Brooks, R. At the MIT Artificial Intelligence Laboratory, which allowed for the design of various levels of intelligence in autonomous robots by building the control system in layers. Each layer is composed of simple computational modules communicating over low bandwidth channels asynchronously. The problem formulated in the language of control theory is as follows: a group of sensors receive information about the environment which the control system processes, turns into commands, and then sends to the motor controller which moves the robot accordingly.

\subsubsection*{Hierarchical/Deliberative Paradigm}
\subsubsection*{The Reactive Paradigm}
\subsubsection*{Hybrid Deliberate/Reactive Paradigm}
\subsubsection*{Automatic Planning}

\subsection*{Choice of Sensors}
The sensors implemented in this design will be used mostly for obstacle detection and navigation.  Some of the most commonly used sensors for these tasks include distance sensors and proximity sensors.  These types of sensors are used to determine the relative distance between the sensor and objects in the environment and can be either passive or active. 

The main type of passive sensor used in object detection is vision based, like a visible spectrum camera.  The images captured by this type of system provide a large amount of information and are easy to interpret. Since passive sensors do not emit any signals, the environmental interference issues that can arise with active sensors are not present.  Although passive sensors are advantageous in this regard, they perform poorly in low light environments since they must rely on natural energy in the environment.  Alternatively, active sensors provide their own energy source for object detection and dramatically outperform passive systems in environments lacking illumination.  Radar, ladar, and sonar are the three main types of active sensors that can be implemented in an object detection system, and all implement a system that emits a signal and interprets reflections to determine information about an object.  \cite{Discant}

Infrared sensors, available as passive or active sensors, are very widely used and offer many advantages over other types of sensors when performing navigation, object avoidance, and line following.  Infrared sensors are also cheap in comparison to other sensor choices and are able to operate in real-time, making them a popular choice for designs like this one.\cite{Calin}

When designing a sensor system, more than one type of sensor can be used when multiple sensors are to be implemented.  As a result, many designs optimize performance by creating a system which includes both active and passive sensors, thus acheiving the advantages of each and eliminating most of the disadvantages that come from using only one type of sensor.  A design which implements a mix of active and passive sensors improves both system performance and object recognition. \cite{Discant}  

\subsection*{Comparison of Algorithms}

\bibliographystyle{plain}
\bibliography{Biblio}

\end{document}

